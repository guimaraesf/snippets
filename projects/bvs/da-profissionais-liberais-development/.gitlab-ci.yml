# ======================================================================================================================
# Global
# ======================================================================================================================

stages:
  - verify
  - build
  - pre-deploy
  - deploy

variables:
  DOCKER_IMAGE_TAG: "gcr.io/bvs-main-98cb/da-profissionais-liberais"
  IMAGE: "399.0.0"
  PYTHON_IMAGE: "python:3.10.12-slim"

build:
  stage: build
  image:
    name: gcr.io/kaniko-project/executor:debug
    entrypoint: [""]
  script:
    - /kaniko/executor --context=$CI_PROJECT_DIR --dockerfile=$CI_PROJECT_DIR/Dockerfile --destination=$DOCKER_IMAGE_TAG:$CI_COMMIT_SHORT_SHA --destination=$DOCKER_IMAGE_TAG

# ======================================================================================================================
# Verify Stage
# ======================================================================================================================
include:
  - project: 'tech/architecture/devsecops'
    file: 'stages/verify/code-quality.yml'
    rules:
      - if: '$CI_COMMIT_BRANCH == "main"'
  - project: 'tech/architecture/devsecops'
    file: 'stages/verify/sast.yml'
    rules:
      - if: '$CI_COMMIT_BRANCH == "main"'

# ======================================================================================================================
# Tests Stage
# ======================================================================================================================
# tests:
#   stage: tests
#   image: $PYTHON_IMAGE
#   before_script:
#     - pip install --no-cache-dir --trusted-host pypi.org --trusted-host files.pythonhosted.org -r requirements.txt
#     - export PYTHONPATH=$(pwd) PROJECT_ID=$PROJECT_ID_DEV BUCKET_ID=$BUCKET_ID_DEV
#   script:
#     - python -m unittest discover --start-directory src/tests --verbose --failfast --pattern '*_test.py'

# ======================================================================================================================
# Build Docs Stage
# ======================================================================================================================
# build_docs:
#   stage: pre-deploy
#   image: $PYTHON_IMAGE
#   before_script:
#     - pip install --no-cache-dir --trusted-host pypi.org --trusted-host files.pythonhosted.org -r requirements.txt
#     - pip install --upgrade sphinx furo sphinx_rtd_theme
#     - apt-get update && apt-get install make --no-install-recommends -y
#     - apt-get update && apt-get install -y git
#     - export PYTHONPATH=$(pwd)
#   script:
#     - sphinx-quickstart -q --sep -p 'Profissionais Liberais' -a 'Fernando Theodoro GuimarÃ£es' -v '1.0.0' -l 'pt_BR' docs
#     - sphinx-apidoc -f -e -o $CI_PROJECT_DIR/docs/source/modules -a . 
#     - cp $CI_PROJECT_DIR/.config/conf.py $CI_PROJECT_DIR/docs/source/
#     - cd docs && make html
#   after_script:
#     - mv docs/build/html ./public
#   artifacts:
#     paths:
#     - ./public
#   allow_failure: true

# ======================================================================================================================
# Static Analysis Stage
# ======================================================================================================================
pylint:
  stage: pre-deploy
  image: $PYTHON_IMAGE
  before_script:
    - pip install --no-cache-dir --trusted-host pypi.org --trusted-host files.pythonhosted.org --upgrade pylint
    - export PYTHONPATH=$(pwd)
    - mkdir pylint
  script:
    - pylint src/utils/ci/*.py src/utils/dataframe/pandas/*.py src/utils/dataframe/pyspark/*.py src/utils/gcp/storage/*.py src/utils/helpers/*.py src/utils/spark/*.py src/utils/time/*.py src/utils/web_tools/*.py src/pipeline/*.py src/pipeline/extract/*.py src/pipeline/load/*.py src/pipeline/transform/*.py src/interfaces/*.py dags/*.py dags/dependencies/*.py --rcfile=.config/.pylintrc
  after_script:
    - pylint src/utils/ci/*.py src/utils/dataframe/pandas/*.py src/utils/dataframe/pyspark/*.py src/utils/gcp/storage/*.py src/utils/helpers/*.py src/utils/spark/*.py src/utils/time/*.py src/utils/web_tools/*.py src/pipeline/*.py src/pipeline/extract/*.py src/pipeline/load/*.py src/pipeline/transform/*.py src/interfaces/*.py dags/*.py dags/dependencies/*.py --rcfile=.config/.pylintrc --output-format=json > pylint/pylint.json
    - mv pylint ./public
  artifacts:
    paths:
    - ./public
  allow_failure: true

bandit:
  stage: pre-deploy
  image: $PYTHON_IMAGE
  before_script:
    - pip install --no-cache-dir --trusted-host pypi.org --trusted-host files.pythonhosted.org --upgrade bandit
    - mkdir bandit
  script:
    - bandit -r .
  after_script:
    - bandit -r -f json -o bandit/bandit.json .
    - mv bandit ./public
  artifacts:
    paths:
    - ./public
  allow_failure: true

# coverage:
#   stage: pre-deploy
#   image: $PYTHON_IMAGE
#   before_script:
#     - pip install --no-cache-dir --trusted-host pypi.org --trusted-host files.pythonhosted.org -r requirements.txt
#     - pip install --no-cache-dir --trusted-host pypi.org --trusted-host files.pythonhosted.org --upgrade coverage
#     - export PYTHONPATH=$(pwd)
#     - mkdir coverage
#   script:
#       - coverage run --source $CI_PROJECT_DIR -m unittest discover --start-directory src/tests --verbose --failfast --pattern *_test.py
#       - coverage report -m
#   after_script:
#     - coverage html -d coverage/coverage.html
#     - coverage json -o coverage/coverage.json
#     - mv coverage ./public
#   artifacts:
#     paths:
#     - ./public
#   allow_failure: true

radon:
  stage: pre-deploy
  image: $PYTHON_IMAGE
  before_script:
    - pip install --no-cache-dir --trusted-host pypi.org --trusted-host files.pythonhosted.org --upgrade radon
    - mkdir radon
  script:
    - radon cc $CI_PROJECT_DIR
    - radon mi $CI_PROJECT_DIR
  after_script:
    - radon cc -s $CI_PROJECT_DIR > radon/report_cc.txt
    - radon mi -s $CI_PROJECT_DIR > radon/report_mi.txt
    - mv radon ./public
  artifacts:
    paths:
      - ./public
  allow_failure: true


# ======================================================================================================================
#                                                   DEVELOPMENT
# ======================================================================================================================
upload-files-to-gcs-dev:
  stage: deploy
  image: google/cloud-sdk:$IMAGE
  before_script:
    - export PYTHONPATH=$(pwd) BUCKET_ID=$BUCKET_ID_DEV PROJECT_ID=$PROJECT_ID_DEV AIRFLOW_CONFIG_DEV=$AIRFLOW_CONFIG_DEV
    - mkdir -p $CI_PROJECT_DIR/dags/dependencies/config && echo $AIRFLOW_CONFIG_DEV >> $CI_PROJECT_DIR/dags/dependencies/config/variables.json
    - apt-get update && apt-get install -y jq && jq . $CI_PROJECT_DIR/dags/dependencies/config/variables.json
    - pip install --no-cache-dir --trusted-host pypi.org --trusted-host files.pythonhosted.org -r requirements.txt
    - pip install --upgrade google-cloud-storage
    - gcloud config set project $PROJECT_ID_DEV
    - gcloud auth login $SERVICE_ACCOUNT_DEV --no-launch-browser && echo 'Y'
    # - echo "Excluding python files from folders DAGS and scripts to Cloud Storage."
    # - gsutil -m rm -r gs://$BUCKET_ID_DEV/scripts/src
    # - gsutil -m rm -r gs://$BUCKET_COMPOSER_ID_DEV/dags/dados_alternativos/profissionais_liberais
    # - gsutil -m rm -r gs://$BUCKET_COMPOSER_ID_DEV/dags/dependencies/dados_alternativos/profissionais_liberais
  script:
    - echo "Uploading python files from src to Cloud Storage."
    - gsutil -m cp -r src/ gs://$BUCKET_ID_DEV/scripts
    - echo "Uploading DAGS from dags to Cloud Composer."
    - gsutil -m cp -r dags/*.py gs://$BUCKET_COMPOSER_ID_DEV/dags/dados_alternativos/profissionais_liberais
    - gsutil -m cp -r dags/dependencies/ gs://$BUCKET_COMPOSER_ID_DEV/dags/dependencies/dados_alternativos/profissionais_liberais
    - gsutil -m cp -r dags/dependencies/config/variables.json gs://$BUCKET_COMPOSER_ID_DEV/dags/dependencies/dados_alternativos/profissionais_liberais/config
    - echo "Uploading shell scripts from .config to Cloud Storage."
    - gsutil -m cp -r .config/*.sh gs://$BUCKET_ID_DEV/scripts/config
    - python3 $CI_PROJECT_DIR/src/utils/ci/upload_files_bucket.py
  only:
    - development


# ======================================================================================================================
#                                                   HOMOLOG
# ======================================================================================================================
upload-files-to-gcs-np:
  stage: deploy
  image: google/cloud-sdk:$IMAGE
  before_script:
    - export PYTHONPATH=$(pwd) BUCKET_ID=$BUCKET_ID_NP PROJECT_ID=$PROJECT_ID_NP AIRFLOW_CONFIG_NP=$AIRFLOW_CONFIG_NP
    - mkdir -p $CI_PROJECT_DIR/dags/dependencies/config && echo $AIRFLOW_CONFIG_NP >> $CI_PROJECT_DIR/dags/dependencies/config/variables.json
    - apt-get update && apt-get install -y jq && jq . $CI_PROJECT_DIR/dags/dependencies/config/variables.json
    - pip install --no-cache-dir --trusted-host pypi.org --trusted-host files.pythonhosted.org -r requirements.txt
    - pip install --upgrade google-cloud-storage
    - gcloud config set project $PROJECT_ID_NP
    - gcloud auth login $SERVICE_ACCOUNT_NP --no-launch-browser && echo 'Y'
    # - echo "Excluding python files from folders DAGS and scripts to Cloud Storage."
    # - gsutil -m rm -r gs://$BUCKET_ID_NP/scripts/src
    # - gsutil -m rm -r gs://$BUCKET_COMPOSER_ID_NP/dags/dados_alternativos/profissionais_liberais
    # - gsutil -m rm -r gs://$BUCKET_COMPOSER_ID_NP/dags/dependencies/dados_alternativos/profissionais_liberais
  script:
    - echo "Uploading python files from src to Cloud Storage."
    - gsutil -m cp -r src/ gs://$BUCKET_ID_NP/scripts
    - echo "Uploading DAGS from dags to Cloud Composer."
    - gsutil -m cp -r dags/*.py gs://$BUCKET_COMPOSER_ID_NP/dags/dados_alternativos/profissionais_liberais
    - gsutil -m cp -r dags/dependencies/ gs://$BUCKET_COMPOSER_ID_NP/dags/dependencies/dados_alternativos/profissionais_liberais
    - gsutil -m cp -r dags/dependencies/config/variables.json gs://$BUCKET_COMPOSER_ID_NP/dags/dependencies/dados_alternativos/profissionais_liberais/config
    - echo "Uploading shell scripts from .config to Cloud Storage."
    - gsutil -m cp -r .config/*.sh gs://$BUCKET_ID_NP/scripts/config
    - python3 $CI_PROJECT_DIR/src/utils/ci/upload_files_bucket.py
  only:
    - homolog

# ======================================================================================================================
#                                                   PRODUCTION
# ======================================================================================================================
upload-files-to-gcs-prod:
  stage: deploy
  image: google/cloud-sdk:$IMAGE
  before_script:
    - export PYTHONPATH=$(pwd) BUCKET_ID=$BUCKET_ID_PROD PROJECT_ID=$PROJECT_ID_PROD AIRFLOW_CONFIG_PROD=$AIRFLOW_CONFIG_PROD
    - mkdir -p $CI_PROJECT_DIR/dags/dependencies/config && echo $AIRFLOW_CONFIG_PROD >> $CI_PROJECT_DIR/dags/dependencies/config/variables.json
    - apt-get update && apt-get install -y jq && jq . $CI_PROJECT_DIR/dags/dependencies/config/variables.json
    - pip install --no-cache-dir --trusted-host pypi.org --trusted-host files.pythonhosted.org -r requirements.txt
    - pip install --upgrade google-cloud-storage
    - gcloud config set project $PROJECT_ID_PROD
    - gcloud auth login $SERVICE_ACCOUNT_PROD --no-launch-browser && echo 'Y'
    # - echo "Excluding python files from folders DAGS and scripts to Cloud Storage."
    # - gsutil -m rm -r gs://$BUCKET_ID_PROD/scripts/src
    # - gsutil -m rm -r gs://$BUCKET_COMPOSER_ID_PROD/dags/dados_alternativos/profissionais_liberais
    # - gsutil -m rm -r gs://$BUCKET_COMPOSER_ID_PROD/dags/dependencies/dados_alternativos/profissionais_liberais
  script:
    - echo "Uploading python files from src to Cloud Storage."
    - gsutil -m cp -r src/ gs://$BUCKET_ID_PROD/scripts
    - echo "Uploading DAGS from dags to Cloud Composer."
    - gsutil -m cp -r dags/*.py gs://$BUCKET_COMPOSER_ID_PROD/dags/dados_alternativos/profissionais_liberais
    - gsutil -m cp -r dags/dependencies/ gs://$BUCKET_COMPOSER_ID_PROD/dags/dependencies/dados_alternativos/profissionais_liberais
    - gsutil -m cp -r dags/dependencies/config/variables.json gs://$BUCKET_COMPOSER_ID_PROD/dags/dependencies/dados_alternativos/profissionais_liberais/config
    - echo "Uploading shell scripts from .config to Cloud Storage."
    - gsutil -m cp -r .config/*.sh gs://$BUCKET_ID_PROD/scripts/config
    - python3 $CI_PROJECT_DIR/src/utils/ci/upload_files_bucket.py
  only:
    - main